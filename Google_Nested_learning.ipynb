{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMIRMOHAMMAD-OSS/Google_NestedLearning/blob/main/Google_Nested_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-intro"
      },
      "source": [
        "# NL-HOPE: Nested Learning & HOPE-style Sequence Model (Self-Contained Colab)\n",
        "\n",
        "This notebook is **self-contained**:\n",
        "- All code (model, data pipeline, training loop, sampling) lives **inside the notebook**.\n",
        "- No `git clone`, no `import nl`, no external repo required.\n",
        "\n",
        "What you can do here:\n",
        "1. Pick **any Hugging Face text dataset** (WikiText-2, IMDB, AG News, etc.).\n",
        "2. Train a compact **HOPE-style model** (fast associative memory + Continuum Memory System).\n",
        "3. Monitor perplexity on a validation split.\n",
        "4. Generate text samples from your trained model.\n",
        "\n",
        "You only need to edit a small configuration block to point to your dataset and tweak model size / steps for your GPU budget."
      ],
      "id": "title-intro"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-explain"
      },
      "source": [
        "## 0. Install dependencies\n",
        "\n",
        "This installs the libraries we use:\n",
        "- **PyTorch** (for the model & training loop)\n",
        "- **transformers** (for tokenization)\n",
        "- **datasets** (for loading Hugging Face datasets)\n",
        "- **tqdm** (for progress bars)"
      ],
      "id": "install-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "!pip install -q torch torchvision torchaudio transformers datasets tqdm"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "install-deps"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-explain"
      },
      "source": [
        "## 1. Imports & configuration\n",
        "\n",
        "In this cell you:\n",
        "- Import all required Python modules.\n",
        "- Define a `Config` dataclass that controls **dataset**, **tokenizer**, **model size**, and **training hyperparameters**.\n",
        "\n",
        "### How to train on your own HF dataset\n",
        "Change these fields in `Config` (below):\n",
        "\n",
        "- `dataset_name`: the Hugging Face dataset id, e.g.\n",
        "  - `\"wikitext\"`\n",
        "  - `\"imdb\"`\n",
        "  - `\"ag_news\"`\n",
        "  - `\"bookcorpusopen\"`\n",
        "- `dataset_config`: dataset configuration (if applicable), e.g.\n",
        "  - WikiText-2: `\"wikitext-2-raw-v1\"`\n",
        "  - IMDB: `\"plain_text\"`\n",
        "  - AG News: `\"default\"` or `\"\"` depending on HF page\n",
        "- `text_field`: the column name that contains the raw text, e.g. `\"text\"`, `\"content\"`, `\"review\"`, etc.\n",
        "\n",
        "You can also tweak:\n",
        "- `block_size`: effective sequence length + 1 (we use 128 context by default).\n",
        "- `d_model`, `cms_dffs`, `n_layers`: model size / capacity.\n",
        "- `max_steps`, `batch_size`: how long and how heavy training is."
      ],
      "id": "config-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports-config"
      },
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # ----------------------\n",
        "    # Dataset (Hugging Face)\n",
        "    # ----------------------\n",
        "    # Examples:\n",
        "    #   WikiText-2: dataset_name=\"wikitext\", dataset_config=\"wikitext-2-raw-v1\", text_field=\"text\"\n",
        "    #   IMDB:      dataset_name=\"imdb\",    dataset_config=\"plain_text\",       text_field=\"text\"\n",
        "    #   AG News:   dataset_name=\"ag_news\", dataset_config=\"\" or \"default\",    text_field=\"text\"\n",
        "    dataset_name: str = \"wikitext\"\n",
        "    dataset_config: str = \"wikitext-2-raw-v1\"\n",
        "    text_field: str = \"text\"\n",
        "\n",
        "    # ----------------------\n",
        "    # Tokenizer\n",
        "    # ----------------------\n",
        "    tokenizer_name: str = \"gpt2\"  # any compatible HF tokenizer\n",
        "    # block_size = context length + 1 target token; we use 128 context\n",
        "    block_size: int = 129\n",
        "\n",
        "    # ----------------------\n",
        "    # Model (HOPE-style)\n",
        "    # ----------------------\n",
        "    vocab_size: int = 50257  # will be overwritten after tokenizer loads\n",
        "    d_model: int = 256       # hidden size\n",
        "    d_kv: int = 64           # key/value dim for fast memory\n",
        "    cms_dffs: tuple = (512, 1024)  # feedforward sizes for CMS levels\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.1\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "    # ----------------------\n",
        "    # Training\n",
        "    # ----------------------\n",
        "    batch_size: int = 2\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.1\n",
        "    max_steps: int = 500\n",
        "    warmup_steps: int = 50\n",
        "    log_every: int = 10\n",
        "    eval_every: int = 100\n",
        "\n",
        "    # ----------------------\n",
        "    # Sampling\n",
        "    # ----------------------\n",
        "    temperature: float = 0.8\n",
        "    top_k: int = 50\n",
        "    top_p: float = 0.95\n",
        "\n",
        "    # ----------------------\n",
        "    # Misc\n",
        "    # ----------------------\n",
        "    seed: int = 1337\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(\"Using device:\", cfg.device)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports-config"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seed-explain"
      },
      "source": [
        "## 2. Set random seed (optional but recommended)\n",
        "\n",
        "This just makes your runs **more reproducible** across restarts (within the usual GPU randomness limits)."
      ],
      "id": "seed-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seed-fn"
      },
      "source": [
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(cfg.seed)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "seed-fn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenizer-dataset-explain"
      },
      "source": [
        "## 3. Load tokenizer & Hugging Face dataset\n",
        "\n",
        "This cell:\n",
        "- Loads the tokenizer (`cfg.tokenizer_name`),\n",
        "- Ensures we have EOS / PAD tokens,\n",
        "- Loads the HF dataset specified in `Config`,\n",
        "- Ensures there is a `train` and `validation` split (creates one if necessary).\n",
        "\n",
        "If you change `dataset_name`, `dataset_config`, or `text_field` in the config, **this is the cell that will reflect it**."
      ],
      "id": "tokenizer-dataset-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "data-tokenizer"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n",
        "\n",
        "# Ensure EOS + PAD tokens\n",
        "if tokenizer.eos_token is None:\n",
        "    tokenizer.eos_token = tokenizer.sep_token or tokenizer.cls_token or tokenizer.pad_token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "cfg.vocab_size = tokenizer.vocab_size\n",
        "print(\"Vocab size:\", cfg.vocab_size)\n",
        "\n",
        "\n",
        "def load_text_dataset(cfg: Config):\n",
        "    \"\"\"Load a HF dataset and ensure we have train + validation splits.\"\"\"\n",
        "    if cfg.dataset_config:\n",
        "        ds = load_dataset(cfg.dataset_name, cfg.dataset_config)\n",
        "    else:\n",
        "        ds = load_dataset(cfg.dataset_name)\n",
        "\n",
        "    if \"train\" not in ds:\n",
        "        # Fallback: split whatever we have into train/validation\n",
        "        ds = ds.train_test_split(test_size=0.1, seed=cfg.seed)\n",
        "        ds[\"validation\"] = ds[\"test\"]\n",
        "        del ds[\"test\"]\n",
        "    return ds\n",
        "\n",
        "\n",
        "raw_datasets = load_text_dataset(cfg)\n",
        "print(raw_datasets)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "data-tokenizer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blocks-explain"
      },
      "source": [
        "## 4. Convert raw text into LM training blocks\n",
        "\n",
        "We now:\n",
        "- Concatenate all texts into one long stream of tokens.\n",
        "- Chop that stream into **fixed-length blocks** of `block_size` tokens.\n",
        "- For each block, we create `(x, y)` where `y` is `x` shifted one position to the left (standard language modeling setup).\n",
        "\n",
        "Key points:\n",
        "- `cfg.block_size` controls how long sequences are (including the target token).\n",
        "- The dataset class `LMDataset` is simple and can be reused if you want to tweak tokenization."
      ],
      "id": "blocks-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset-class"
      },
      "source": [
        "class LMDataset(Dataset):\n",
        "    \"\"\"Simple blockwise language modeling dataset.\n",
        "\n",
        "    token_ids: 1D list of token ids.\n",
        "    block_size: sequence length for x (we internally keep block_size+1 to form targets).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, token_ids, block_size: int):\n",
        "        self.block_size = block_size\n",
        "        num_blocks = (len(token_ids) - 1) // block_size\n",
        "        self.data = []\n",
        "        for i in range(num_blocks):\n",
        "            start = i * block_size\n",
        "            end = start + block_size\n",
        "            block = token_ids[start:end + 1]  # +1 for targets shift\n",
        "            if len(block) == block_size + 1:\n",
        "                self.data.append(block)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        block = torch.tensor(self.data[idx], dtype=torch.long)\n",
        "        x = block[:-1]\n",
        "        y = block[1:]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def tokenize_and_build_dataset(split_dataset, cfg: Config):\n",
        "    \"\"\"Tokenize all examples in a split into one long stream and pack into blocks.\"\"\"\n",
        "    all_texts = []\n",
        "    for ex in split_dataset:\n",
        "        txt = ex.get(cfg.text_field, \"\")\n",
        "        if txt is None:\n",
        "            continue\n",
        "        all_texts.append(txt)\n",
        "    joined = \"\\n\".join(all_texts)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        joined,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=False,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    ids = enc[\"input_ids\"]\n",
        "    if isinstance(ids[0], list):\n",
        "        flat = [t for seq in ids for t in seq]\n",
        "    else:\n",
        "        flat = list(ids)\n",
        "\n",
        "    # We reserve 1 token for the target shift; x length = block_size-1\n",
        "    return LMDataset(flat, cfg.block_size - 1)\n",
        "\n",
        "\n",
        "train_dataset = tokenize_and_build_dataset(raw_datasets[\"train\"], cfg)\n",
        "val_dataset = tokenize_and_build_dataset(raw_datasets[\"validation\"], cfg)\n",
        "print(\"Train blocks:\", len(train_dataset))\n",
        "print(\"Val blocks:\", len(val_dataset))\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)\n",
        "    y = torch.stack(ys, dim=0)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"Batch shapes:\", xb.shape, yb.shape)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dataset-class"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-explain"
      },
      "source": [
        "## 5. Define the HOPE-style model (FastKV + CMS)\n",
        "\n",
        "Here we implement a simplified HOPE-style sequence model:\n",
        "\n",
        "- **FastKVAttention**: a linear/fast attention-like associative memory. It compresses keys and values into a global state and retrieves values using a feature map \\(\\phi(\\cdot)\\).\n",
        "- **CMSLayer**: one level of the **Continuum Memory System**, implemented as a residual MLP with LayerNorm.\n",
        "- **HOPEBlock**: one block that combines FastKV + a chain of CMS layers.\n",
        "- **HOPEModel**: token & positional embeddings, a stack of HOPEBlocks, and a LM head over the vocabulary.\n",
        "\n",
        "This captures the high-level idea of **Nested Learning** / **continuum memory** in a compact, Colab-friendly model."
      ],
      "id": "model-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model-cell"
      },
      "source": [
        "class FastKVAttention(nn.Module):\n",
        "    \"\"\"Simple fast associative memory approximating linear attention.\n",
        "\n",
        "    We use a feature map phi(x) = ReLU(x) + 1 and compute:\n",
        "      KV = sum_t phi(k_t)^T v_t\n",
        "      y_t = (phi(q_t) @ KV) / (phi(q_t) @ sum_t phi(k_t) + eps)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, d_kv: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(d_model, d_kv)\n",
        "        self.k_proj = nn.Linear(d_model, d_kv)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def phi(self, x):\n",
        "        return F.relu(x) + 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, d_model)\n",
        "        B, T, D = x.shape\n",
        "        q = self.q_proj(x)  # (B, T, d_kv)\n",
        "        k = self.k_proj(x)  # (B, T, d_kv)\n",
        "        v = self.v_proj(x)  # (B, T, d_model)\n",
        "\n",
        "        q_phi = self.phi(q)           # (B, T, d_kv)\n",
        "        k_phi = self.phi(k)           # (B, T, d_kv)\n",
        "\n",
        "        # Sum over keys\n",
        "        k_sum = k_phi.sum(dim=1)      # (B, d_kv)\n",
        "\n",
        "        # KV = sum_t phi(k_t)^T v_t\n",
        "        KV = torch.einsum(\"btd,btm->bdm\", k_phi, v)  # (B, d_kv, d_model)\n",
        "\n",
        "        # Numerator: phi(q_t) @ KV\n",
        "        num = torch.einsum(\"btd,bdm->btm\", q_phi, KV)  # (B, T, d_model)\n",
        "        # Denominator: phi(q_t) @ k_sum\n",
        "        denom = (q_phi * k_sum.unsqueeze(1)).sum(dim=-1, keepdim=True)  # (B, T, 1)\n",
        "        eps = 1e-6\n",
        "        y = num / (denom + eps)\n",
        "        y = self.dropout(self.out_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class CMSLayer(nn.Module):\n",
        "    \"\"\"One level in the Continuum Memory System (CMS).\n",
        "\n",
        "    Implemented as a standard residual MLP (LN -> Linear -> GELU -> Linear -> Dropout + Skip).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.ln(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return residual + x\n",
        "\n",
        "\n",
        "class HOPEBlock(nn.Module):\n",
        "    \"\"\"One HOPE-style block: FastKV + multi-level CMS.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, d_kv: int, cms_dffs, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.ln_attn = nn.LayerNorm(d_model)\n",
        "        self.attn = FastKVAttention(d_model, d_kv, dropout=dropout)\n",
        "        self.cms_layers = nn.ModuleList([\n",
        "            CMSLayer(d_model, d_ff, dropout=dropout) for d_ff in cms_dffs\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # FastKV attention (working memory)\n",
        "        attn_in = self.ln_attn(x)\n",
        "        attn_out = self.attn(attn_in)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # CMS chain (continuum long-term-ish memory)\n",
        "        for cms in self.cms_layers:\n",
        "            x = cms(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HOPEModel(nn.Module):\n",
        "    \"\"\"HOPE-style LM: embeddings + HOPEBlocks + LM head.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            HOPEBlock(cfg.d_model, cfg.d_kv, cfg.cms_dffs, dropout=cfg.dropout)\n",
        "            for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
        "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        B, T = x.shape\n",
        "        if T > self.cfg.max_seq_len:\n",
        "            x = x[:, -self.cfg.max_seq_len:]\n",
        "            T = x.shape[1]\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
        "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(h)\n",
        "\n",
        "        h = self.ln_f(h)\n",
        "        logits = self.head(h)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = HOPEModel(cfg).to(cfg.device)\n",
        "print(\"Model params:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "model-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-explain"
      },
      "source": [
        "## 6. Train the model (language modeling)\n",
        "\n",
        "We now:\n",
        "- Use **cross-entropy LM loss** on the next token.\n",
        "- Apply a simple linear **warmup schedule** for the learning rate.\n",
        "- Log loss and approximate perplexity every `cfg.log_every` steps.\n",
        "- Evaluate full validation perplexity every `cfg.eval_every` steps.\n",
        "\n",
        "You can control training cost by tweaking:\n",
        "- `cfg.max_steps` (total optimization steps),\n",
        "- `cfg.batch_size` (watch Colab GPU memory),\n",
        "- `cfg.d_model`, `cfg.n_layers`, and `cfg.cms_dffs` (model size)."
      ],
      "id": "train-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train-eval"
      },
      "source": [
        "def get_lr(step: int, cfg: Config):\n",
        "    if step < cfg.warmup_steps:\n",
        "        return cfg.lr * float(step + 1) / float(cfg.warmup_steps)\n",
        "    return cfg.lr\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_ppl(model, data_loader, cfg: Config):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    for x, y in data_loader:\n",
        "        x = x.to(cfg.device)\n",
        "        y = y.to(cfg.device)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            y.reshape(-1),\n",
        "            reduction=\"sum\",\n",
        "        )\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += y.numel()\n",
        "    avg_nll = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_nll)\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, cfg: Config):\n",
        "    global_step = 0\n",
        "    pbar = tqdm(total=cfg.max_steps, desc=\"training steps\")\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    while global_step < cfg.max_steps:\n",
        "        for x, y in train_loader:\n",
        "            if global_step >= cfg.max_steps:\n",
        "                break\n",
        "\n",
        "            x = x.to(cfg.device)\n",
        "            y = y.to(cfg.device)\n",
        "\n",
        "            lr = get_lr(global_step, cfg)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group[\"lr\"] = lr\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                y.reshape(-1),\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            running_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "            if global_step % cfg.log_every == 0:\n",
        "                avg_loss = running_loss / cfg.log_every\n",
        "                ppl = math.exp(min(20.0, avg_loss))\n",
        "                pbar.write(f\"step={global_step} loss={avg_loss:.4f} pplâ‰ˆ{ppl:.2f} lr={lr:.2e}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "            if global_step % cfg.eval_every == 0:\n",
        "                val_ppl = evaluate_ppl(model, val_loader, cfg)\n",
        "                pbar.write(f\"[eval] step={global_step} val_ppl={val_ppl:.2f}\")\n",
        "\n",
        "            if global_step >= cfg.max_steps:\n",
        "                break\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "\n",
        "train(model, train_loader, val_loader, cfg)\n",
        "final_val_ppl = evaluate_ppl(model, val_loader, cfg)\n",
        "print(\"Final validation perplexity:\", final_val_ppl)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "train-eval"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sampling-explain"
      },
      "source": [
        "## 7. Generate text from the trained model\n",
        "\n",
        "This cell implements a simple sampler with:\n",
        "- **Temperature** scaling (`cfg.temperature`)\n",
        "- Optional **top-k** filtering (`cfg.top_k`)\n",
        "- Optional **top-p** (nucleus) filtering (`cfg.top_p`)\n",
        "\n",
        "You can change the `prompt` string and the `max_new_tokens` argument to explore the model's behavior after training."
      ],
      "id": "sampling-explain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sampling"
      },
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, tokenizer, prompt: str, cfg: Config, max_new_tokens: int = 50):\n",
        "    model.eval()\n",
        "    device = cfg.device\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    x = encoded[\"input_ids\"].to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > cfg.max_seq_len:\n",
        "            x_cond = x[:, -cfg.max_seq_len:]\n",
        "        else:\n",
        "            x_cond = x\n",
        "\n",
        "        logits = model(x_cond)\n",
        "        logits = logits[:, -1, :] / cfg.temperature\n",
        "\n",
        "        # Base probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Top-k filtering\n",
        "        if cfg.top_k > 0:\n",
        "            v, ix = torch.topk(probs, cfg.top_k, dim=-1)\n",
        "            probs_zero = torch.zeros_like(probs).scatter_(-1, ix, v)\n",
        "            probs = probs_zero / probs_zero.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Top-p (nucleus) filtering\n",
        "        if cfg.top_p < 1.0:\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            mask = cumulative_probs > cfg.top_p\n",
        "            mask[..., 1:] = mask[..., :-1].clone()\n",
        "            mask[..., 0] = False\n",
        "            sorted_probs[mask] = 0.0\n",
        "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "            probs_zero = torch.zeros_like(probs).scatter_(-1, sorted_indices, sorted_probs)\n",
        "            probs = probs_zero\n",
        "\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        x = torch.cat([x, next_token], dim=1)\n",
        "\n",
        "    return tokenizer.decode(x[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "\n",
        "prompt = \"Nested learning suggests\"\n",
        "generated = sample(model, tokenizer, prompt, cfg, max_new_tokens=100)\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "sampling"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adapt-explain"
      },
      "source": [
        "## 8. How to adapt this notebook to **your** dataset\n",
        "\n",
        "You mostly only need to touch the `Config` cell at the top.\n",
        "\n",
        "### A. Using a different Hugging Face dataset\n",
        "\n",
        "1. Go to [https://huggingface.co/datasets](https://huggingface.co/datasets) and pick a dataset.\n",
        "2. Note its `dataset_name`, optional `dataset_config`, and the column name that holds text.\n",
        "3. In the `Config` class, change:\n",
        "   - `dataset_name` (e.g. `\"imdb\"`),\n",
        "   - `dataset_config` (e.g. `\"plain_text\"` or `\"\"`),\n",
        "   - `text_field` (e.g. `\"text\"`, `\"content\"`, `\"review\"`).\n",
        "4. Re-run the notebook from **top to bottom**.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- **IMDB reviews**\n",
        "  ```python\n",
        "  dataset_name = \"imdb\"\n",
        "  dataset_config = \"plain_text\"\n",
        "  text_field = \"text\"\n",
        "  ```\n",
        "\n",
        "- **AG News**\n",
        "  ```python\n",
        "  dataset_name = \"ag_news\"\n",
        "  dataset_config = \"\"\n",
        "  text_field = \"text\"\n",
        "  ```\n",
        "\n",
        "If you hit out-of-memory issues, try:\n",
        "- Reducing `d_model`, `cms_dffs`, or `n_layers`.\n",
        "- Reducing `batch_size`.\n",
        "- Reducing `max_seq_len` / `block_size`.\n",
        "\n",
        "### B. Using your own local/raw text\n",
        "\n",
        "A simple route is:\n",
        "\n",
        "1. Prepare one or more `.txt` files and upload them to Colab (e.g. via the file browser).\n",
        "2. Instead of using a HF dataset, you can build your own `raw_datasets` dictionary using `datasets.load_dataset(\"text\", data_files=...)` in the dataset cell.\n",
        "3. Make sure you adapt `text_field` and the tokenization step accordingly.\n",
        "\n",
        "Once your text is exposed as a list of strings, you can **reuse the same LMDataset + training + sampling code** without any further changes."
      ],
      "id": "adapt-explain"
    }
  ]
}