{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "NL-HOPE Nested Learning (Colab)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-intro"
      },
      "source": [
        "# NL-HOPE: Nested Learning & HOPE Sequence Model (Colab)\n",
        "\n",
        "This notebook shows how to:\n",
        "\n",
        "1. Set up the **NL-HOPE** PyTorch implementation.\n",
        "2. Train the HOPE model on **any Hugging Face text dataset** (or on WikiText-2 by default).\n",
        "3. Evaluate perplexity.\n",
        "4. Generate samples from a trained checkpoint.\n",
        "\n",
        "Repository: `https://github.com/AMIRMOHAMMAD-OSS/Google_NestedLearning`\n",
        "\n",
        "> **Tip:** You can copy this notebook file into your repo as `notebooks/NL_HOPE_colab.ipynb` and add an \"Open in Colab\" badge in your README."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "check-gpu"
      },
      "source": [
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"No GPU detected. Training will be slow. In Colab, go to Runtime -> Change runtime type -> GPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## 1. Setup: Clone repo & install dependencies\n",
        "\n",
        "This cell will:\n",
        "\n",
        "- Clone the `Google_NestedLearning` repository (if not already cloned)\n",
        "- Install the Python dependencies listed in `requirements.txt`\n",
        "- Set `PYTHONPATH` so that `import nl` works correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone-install"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "%cd /content\n",
        "\n",
        "if not os.path.exists(\"Google_NestedLearning\"):\n",
        "    !git clone https://github.com/AMIRMOHAMMAD-OSS/Google_NestedLearning.git\n",
        "else:\n",
        "    print(\"Repo already cloned.\")\n",
        "\n",
        "%cd /content/Google_NestedLearning\n",
        "%env PYTHONPATH=/content/Google_NestedLearning\n",
        "\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"Setup complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-config-intro"
      },
      "source": [
        "## 2. Choose your dataset\n",
        "\n",
        "This implementation uses **Hugging Face Datasets** under the hood. You have two main options:\n",
        "\n",
        "1. **Use a built-in HF dataset** (recommended)\n",
        "   - e.g. `wikitext`, `imdb`, `bookcorpus`, or any other dataset whose samples contain at least one text field.\n",
        "2. **Wrap your own data as a HF dataset**\n",
        "   - e.g. upload text files, then use `datasets.load_dataset(\"text\", data_files=...)` in your own script, and point the config at that dataset.\n",
        "\n",
        "In this notebook, we will generate a simple **data config** YAML file that tells the training script which dataset and text field to use.\n",
        "\n",
        "> By default, we use **WikiText-2** with GPT-2 tokenizer, which is a safe, small starting point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "write-data-config"
      },
      "source": [
        "%%writefile configs/data_custom.yaml\n",
        "data:\n",
        "  # Hugging Face tokenizer name\n",
        "  tokenizer_name: gpt2\n",
        "\n",
        "  # Hugging Face dataset name and config\n",
        "  # Example: WikiText-2\n",
        "  dataset_name: wikitext\n",
        "  dataset_config: wikitext-2-raw-v1\n",
        "\n",
        "  # Name of the text field in the dataset\n",
        "  text_field: text\n",
        "\n",
        "  # You can change these to use another dataset. For example:\n",
        "  #  - dataset_name: imdb\n",
        "  #  - dataset_config: plain_text\n",
        "  #  - text_field: text\n",
        "  # Make sure the dataset exists on Hugging Face and that `text_field` matches a column name."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-config-intro"
      },
      "source": [
        "## 3. Model & training config (HOPE + CMS)\n",
        "\n",
        "Next, we create a model/training config suitable for Colab.\n",
        "\n",
        "- **`d_model`**, **`n_layers`**, and **`max_seq_len`** are kept moderate so that training fits on a Colab GPU.\n",
        "- **`max_steps`** is small by default so you can quickly verify everything works.\n",
        "\n",
        "You can always increase these values later for more serious experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "write-model-config"
      },
      "source": [
        "%%writefile configs/hope_colab_user.yaml\n",
        "exp_name: hope_colab_user\n",
        "seed: 1337\n",
        "\n",
        "model:\n",
        "  vocab_size: 50257          # GPT-2 vocab size\n",
        "  d_model: 256               # model hidden size\n",
        "  d_ff: 1024                 # feedforward size for highest CMS level\n",
        "  n_layers: 2                # number of HOPE/CMS layers\n",
        "  max_seq_len: 128           # context length\n",
        "  dropout: 0.1\n",
        "  d_kv: 64                   # key/value dimension for fast memory\n",
        "\n",
        "  # Continuum Memory System (CMS) levels\n",
        "  # Each level has its own feedforward size and update frequency\n",
        "  cms_levels:\n",
        "    - { d_ff: 512,  update_every: 1 }\n",
        "    - { d_ff: 1024, update_every: 8 }\n",
        "\n",
        "  # Inner (fast) learning rule hyperparameters\n",
        "  inner_lr: 5.0e-4\n",
        "  inner_scale_xtx: 1.0\n",
        "  inner_apply_during_eval: true\n",
        "  inner_apply_during_sampling: false\n",
        "\n",
        "train:\n",
        "  batch_size: 2\n",
        "  grad_accum_steps: 1\n",
        "  lr: 3.0e-4\n",
        "  weight_decay: 0.1\n",
        "\n",
        "  # Keep these small at first; increase after you verify training works\n",
        "  max_steps: 500\n",
        "  warmup_steps: 50\n",
        "\n",
        "  eval_every: 100\n",
        "  ckpt_every: 500\n",
        "  log_every: 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-section"
      },
      "source": [
        "## 4. Train HOPE on your chosen dataset\n",
        "\n",
        "Now we call the repository's training script with:\n",
        "\n",
        "- `--config configs/hope_colab_user.yaml` for model & training hyperparameters\n",
        "- `--data configs/data_custom.yaml` for dataset & tokenizer\n",
        "\n",
        "If you changed the dataset in `data_custom.yaml`, it will train on that instead of WikiText-2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run-train"
      },
      "source": [
        "%cd /content/Google_NestedLearning\n",
        "%env PYTHONPATH=/content/Google_NestedLearning\n",
        "\n",
        "!python scripts/train.py \\\n",
        "  --config configs/hope_colab_user.yaml \\\n",
        "  --data configs/data_custom.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval-section"
      },
      "source": [
        "## 5. Evaluate perplexity\n",
        "\n",
        "After training, you can evaluate the model on the validation set using the same data config.\n",
        "\n",
        "This expects a checkpoint at `outputs/hope_colab_user/last.ckpt` (the default location used by the training script)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run-eval"
      },
      "source": [
        "%cd /content/Google_NestedLearning\n",
        "%env PYTHONPATH=/content/Google_NestedLearning\n",
        "\n",
        "ckpt_path = \"outputs/hope_colab_user/last.ckpt\"\n",
        "print(\"Using checkpoint:\", ckpt_path)\n",
        "\n",
        "!python scripts/eval_ppl.py \\\n",
        "  --checkpoint \"$ckpt_path\" \\\n",
        "  --config_model configs/hope_colab_user.yaml \\\n",
        "  --data configs/data_custom.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample-section"
      },
      "source": [
        "## 6. Generate text samples\n",
        "\n",
        "Use `scripts/sample.py` to generate text from your trained HOPE model.\n",
        "\n",
        "- `--prompt` provides a starting text\n",
        "- `--max_new_tokens` controls generation length\n",
        "- You can adjust `temperature`, `top_k`, and `top_p` for different sampling behaviors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run-sample"
      },
      "source": [
        "%cd /content/Google_NestedLearning\n",
        "%env PYTHONPATH=/content/Google_NestedLearning\n",
        "\n",
        "ckpt_path = \"outputs/hope_colab_user/last.ckpt\"\n",
        "prompt = \"Nested learning suggests\"\n",
        "\n",
        "!python scripts/sample.py \\\n",
        "  --checkpoint \"$ckpt_path\" \\\n",
        "  --prompt \"$prompt\" \\\n",
        "  --max_new_tokens 100 \\\n",
        "  --temperature 0.8 \\\n",
        "  --top_k 50 \\\n",
        "  --top_p 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom-datasets-notes"
      },
      "source": [
        "## 7. Training on your own data\n",
        "\n",
        "To train HOPE on **your own dataset**:\n",
        "\n",
        "### Option A: Use an existing Hugging Face dataset\n",
        "\n",
        "1. Find a dataset on [https://huggingface.co/datasets](https://huggingface.co/datasets).\n",
        "2. Identify the `dataset_name`, optional `dataset_config`, and the text column name.\n",
        "3. Edit `configs/data_custom.yaml` and set:\n",
        "   - `dataset_name` to the HF dataset name (e.g. `imdb`, `ag_news`, `bookcorpusopen`)\n",
        "   - `dataset_config` to the appropriate config (or leave blank if none)\n",
        "   - `text_field` to the column containing the text (e.g. `text`, `content`, etc.)\n",
        "4. Rerun the **Train** cell.\n",
        "\n",
        "---\n",
        "\n",        
        "### Option B: Wrap your own text into a HF dataset (advanced)\n",
        "\n",
        "If you have raw text files, one workflow is:\n",
        "\n",
        "1. Create a small Python script or notebook that uses `datasets.load_dataset` with the `\"text\"` builder, for example:\n",
        "\n",
        "   ```python\n",
        "   from datasets import load_dataset\n",
        "\n",
        "   dataset = load_dataset(\n",
        "       \"text\",\n",
        "       data_files={\n",
        "           \"train\": \"path/to/train.txt\",\n",
        "           \"validation\": \"path/to/val.txt\",\n",        
        "           \"test\": \"path/to/test.txt\",\n",
        "       }\n",
        "   )\n",
        "   print(dataset)\n",
        "   ```\n",
        "\n",
        "2. Either:\n",
        "   - Use this custom dataset directly in your own training script, **or**\n",
        "   - Push it to the Hugging Face Hub and reference it with `dataset_name` and `dataset_config` in `configs/data_custom.yaml`.\n",
        "\n",
        "3. Ensure that `text_field` matches the name of the column that contains your text (often `\"text\"`).\n",
        "\n",
        "Once your dataset is exposed via `datasets` in this way, you can train HOPE with the **same training and evaluation commands** used in this notebook."
      ]
    }
  ]
}
